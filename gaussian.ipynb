{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# low-probability estimation using gaussians as the example distribution, and differentiable cost functions\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import tqdm\n",
    "\n",
    "from langevin import sample_langevin_conditional, SampleConfig, AnimateConfig\n",
    "\n",
    "cost_threshold = -7.5\n",
    "\n",
    "def cost_function(xs):\n",
    "    return xs[:, 1].pow(2) + xs[:, 0]\n",
    "\n",
    "def prior_density(xs):\n",
    "    return torch.exp(-xs.pow(2).sum(dim=-1) / 2) / (2 * np.pi) ** (xs.shape[-1] / 2)\n",
    "\n",
    "def log_prior_density(xs):\n",
    "    return -0.5 * xs.pow(2).sum(dim=-1) - np.log(2 * np.pi) * (xs.shape[-1] / 2)\n",
    "\n",
    "img_size = 500\n",
    "\n",
    "full_lims = ((-10, 10), (-10, 10))\n",
    "focus_lims = ((-10, 0), (-5, 5))\n",
    "\n",
    "xs_full, ys_full = torch.meshgrid(torch.linspace(full_lims[0][0], full_lims[0][1], img_size), torch.linspace(full_lims[1][0], full_lims[0][1], img_size))\n",
    "xs_focus, ys_focus = torch.meshgrid(torch.linspace(focus_lims[0][0], focus_lims[0][1], img_size), torch.linspace(focus_lims[1][0], focus_lims[1][1], img_size))\n",
    "\n",
    "# plot density\n",
    "points_full = torch.stack([xs_full, ys_full], dim=-1)\n",
    "\n",
    "density_full = prior_density(points_full)\n",
    "\n",
    "cost_full = cost_function(points_full.reshape(-1, 2)).reshape(points_full.shape[:-1])\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.contourf(xs_full.numpy(), ys_full.numpy(), cost_full.numpy(), levels=20, cmap='inferno')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y') \n",
    "plt.title('Cost Function')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.contourf(xs_full.numpy(), ys_full.numpy(), density_full.numpy(), levels=20, cmap='inferno')\n",
    "plt.colorbar(label='Density')\n",
    "plt.contour(xs_full.numpy(), ys_full.numpy(), cost_full.numpy(), levels=[cost_threshold], colors='red')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y') \n",
    "plt.title('Prior Density')\n",
    "plt.show()\n",
    "\n",
    "points_focus = torch.stack([xs_focus, ys_focus], dim=-1)\n",
    "density_focus = prior_density(points_focus)\n",
    "cost_focus = cost_function(points_focus.reshape(-1, 2)).reshape(points_focus.shape[:-1])\n",
    "\n",
    "conditional_density_focus = density_focus.clone()\n",
    "conditional_density_focus[cost_focus > cost_threshold] = 0\n",
    "conditional_density_focus /= conditional_density_focus.sum() * (focus_lims[0][1] - focus_lims[0][0]) / img_size\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.contourf(xs_focus.numpy(), ys_focus.numpy(), conditional_density_focus.numpy(), levels=20, cmap='inferno')\n",
    "plt.colorbar(label='Density')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y') \n",
    "plt.title('Conditional Density')\n",
    "plt.show()\n",
    "\n",
    "# check that the prior density integrates to 1\n",
    "integral = density_full.sum() * (full_lims[0][1] - full_lims[0][0]) * (full_lims[1][1] - full_lims[1][0]) / (img_size * img_size)\n",
    "print(f\"Integral of density: {integral.item():.6f}\")\n",
    "\n",
    "conditional_density_full = density_full.clone()\n",
    "conditional_density_full[cost_full > cost_threshold] = 0\n",
    "normalizing_constant = conditional_density_full.sum() * (full_lims[0][1] - full_lims[0][0]) * (full_lims[1][1] - full_lims[1][0]) / (img_size * img_size)\n",
    "\n",
    "print(normalizing_constant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_config = SampleConfig(\n",
    "    steps=10_000, \n",
    "    start_beta=1, \n",
    "    end_beta=100,\n",
    "    animate=AnimateConfig(\n",
    "        output_dir=\"langevin_samples\",\n",
    "        capture_every=100,\n",
    "        duration=10000,\n",
    "        size=200,\n",
    "        d_lims=(0, 5),\n",
    "        xlims=(-10, 0),\n",
    "        ylims=(-5, 5)\n",
    "    ),\n",
    "    progress=True\n",
    ")\n",
    "\n",
    "samples = sample_langevin_conditional(\n",
    "    torch.randn(10_000, 2, dtype=torch.float64), \n",
    "    log_prior_density, \n",
    "    cost_function, \n",
    "    cost_threshold, \n",
    "    sample_config\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.hist2d(samples[:, 0].numpy(), samples[:, 1].numpy(), bins=100, cmap='inferno', range=[full_lims[0], full_lims[1]], density=True)\n",
    "plt.colorbar()\n",
    "plt.title('Samples')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_density(xs, ps, bandwidth=1):\n",
    "    # calculate the kernel density estimate of xs given the known samples ps\n",
    "    # xs: (n, d)\n",
    "    # ps: (m, d)\n",
    "    # bandwidth: float\n",
    "\n",
    "    def rbf(a, b, bandwidth):\n",
    "        return torch.exp(-((a - b).pow(2).sum(dim=-1) / (2 * bandwidth**2)))\n",
    "\n",
    "    def normalizing_constant(ps, bandwidth):\n",
    "        n = ps.shape[0]\n",
    "        dim = ps.shape[1]\n",
    "        return (2 * np.pi * bandwidth**2)**(dim / 2) * n\n",
    "\n",
    "    print(xs[:, None, :].shape)\n",
    "    print(ps[None, :, :].shape)\n",
    "\n",
    "    rbfs = rbf(xs[:, None, :], ps[None, :, :], bandwidth).sum(dim=-1)\n",
    "    print(rbfs.shape)\n",
    "    return rbfs / normalizing_constant(ps, bandwidth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xs, ys = torch.meshgrid(torch.linspace(full_lims[0][0], full_lims[0][1], 100), torch.linspace(full_lims[1][0], full_lims[1][1], 100))\n",
    "\n",
    "# grid_points = torch.stack([xs.flatten(), ys.flatten()], dim=1)\n",
    "\n",
    "# print(grid_points.shape)\n",
    "# print(samples.shape)\n",
    "\n",
    "# density = kernel_density(grid_points, samples[len(samples)//2:], bandwidth=0.1)\n",
    "\n",
    "# plt.figure(figsize=(8, 8))\n",
    "# plt.contourf(xs.numpy(), ys.numpy(), density.reshape(100, 100).numpy())\n",
    "# plt.colorbar()\n",
    "\n",
    "# costs = cost_function(grid_points)\n",
    "# plt.contour(xs.numpy(), ys.numpy(), costs.reshape(100, 100).numpy(), levels=[cost_threshold], colors='red')\n",
    "\n",
    "# plt.title('Kernel Density Estimate')\n",
    "# plt.xlabel('x')\n",
    "# plt.ylabel('y')\n",
    "# plt.show()\n",
    "\n",
    "# # Integrate the density over the grid to verify it sums to approximately 1\n",
    "# dx = (full_lims[0][1] - full_lims[0][0]) / (xs.shape[0] - 1)\n",
    "# dy = (full_lims[1][1] - full_lims[1][0]) / (ys.shape[0] - 1)\n",
    "\n",
    "# integral = density.sum() * dx * dy\n",
    "# print(f\"Integral of density: {integral.item():.4f}\")  # Should be close to 1.0\n",
    "\n",
    "# def prior_density(xs):\n",
    "#     return torch.exp(-(xs**2).sum(dim=1) / 2) / np.sqrt(2 * np.pi)\n",
    "\n",
    "# q_values = kernel_density(samples[:len(samples)//2], samples[len(samples)//2:], bandwidth=0.1)\n",
    "# p_values = prior_density(samples[:len(samples)//2])\n",
    "\n",
    "# print((p_values / q_values).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a Gaussian distribution to the samples\n",
    "\n",
    "# but scale up the covariance a bit\n",
    "\n",
    "mean = samples.mean(dim=0)\n",
    "cov = torch.cov(samples.T)\n",
    "# cov *= 2\n",
    "\n",
    "print(\"Fitted Gaussian parameters:\")\n",
    "print(f\"Mean: {mean}\")\n",
    "print(f\"Covariance matrix:\\n{cov}\")\n",
    "\n",
    "# Visualize the fitted Gaussian\n",
    "xs, ys = torch.meshgrid(torch.linspace(full_lims[0][0], full_lims[0][1], 100), \n",
    "                       torch.linspace(full_lims[1][0], full_lims[1][1], 100))\n",
    "grid_points = torch.stack([xs.flatten(), ys.flatten()], dim=1)\n",
    "\n",
    "# Calculate Gaussian density on grid\n",
    "diff = grid_points - mean.unsqueeze(0)\n",
    "inv_cov = torch.inverse(cov)\n",
    "mahalanobis = torch.sum(diff @ inv_cov * diff, dim=1)\n",
    "density = torch.exp(-0.5 * mahalanobis) / (2 * np.pi * torch.sqrt(torch.det(cov)))\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.contourf(xs.numpy(), ys.numpy(), density.reshape(100, 100).numpy())\n",
    "plt.colorbar()\n",
    "plt.title('Fitted Gaussian Distribution')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate samples from the fitted Gaussian (q(x))\n",
    "n_samples = 100_000\n",
    "proposal_samples = torch.distributions.MultivariateNormal(mean, cov).sample((n_samples,)).to(torch.float64)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.hist2d(proposal_samples[:, 0].numpy(), proposal_samples[:, 1].numpy(), bins=100, cmap='inferno', range=[full_lims[0], full_lims[1]], density=True)\n",
    "plt.colorbar()\n",
    "plt.title('Proposal Samples')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()\n",
    "\n",
    "def log_proposal_density(x):\n",
    "    k = x.shape[-1]\n",
    "    log_normalizer = -(k / 2) * np.log(2 * np.pi) - 0.5 * np.log(torch.det(cov))\n",
    "    diff = x - mean.unsqueeze(0)\n",
    "    log_unnormalized = -0.5 * torch.sum(diff @ inv_cov * diff, dim=1)\n",
    "    return log_normalizer + log_unnormalized\n",
    "\n",
    "mask = cost_function(proposal_samples) < cost_threshold\n",
    "log_weights = log_prior_density(proposal_samples[mask]) - log_proposal_density(proposal_samples[mask])\n",
    "max_log_weight = torch.max(log_weights)\n",
    "log_Z = max_log_weight + torch.log(torch.sum(torch.exp(log_weights - max_log_weight)) / n_samples)\n",
    "\n",
    "print(f\"Estimated log normalizing constant: {log_Z.item():.2f}\")\n",
    "print(f\"Estimated normalizing constant: {torch.exp(log_Z).item():.2e}\")\n",
    "print(f\"Actual normalizing constant: {normalizing_constant.item():.2e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
